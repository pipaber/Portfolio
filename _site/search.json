[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Piero Palacios",
    "section": "",
    "text": "Hi! My name is Piero, and I’m a Biologist (with main formation in biotechnology) from Peru. I have specializations in data analysis and bioinformatics from Harvard University and MIT. Also, I love writing reproducible interactive reports using Quarto as a hobby.\nWithin my bioinformatic experience you can find RNA-Seq, Methyl-Seq, WGBS-Seq, Illumina Methylation Array and Multi-omics analyses. Also, I have experience on machine learning (OLS, ridge and lasso regression, SVM, Kernels, collaborative filtering, network analysis, high-dimensional reduction, etc) and deep learning models (Feed forward NN, Recurrent NN, Convolutional NN, Graph NN, Transformers, etc)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "bioinformatics.html",
    "href": "bioinformatics.html",
    "title": "Bioinformatics Projects",
    "section": "",
    "text": "The analysis of RNA-Seq data involves two distinct parts. The first one needs the use of servers or HPC (high performance computers) and has to do with quality control. pre-processing, alignment-mapping (usually with the STAR software) and counting (can be done using RSEM software). The second one is called downstream analysis and this part involves differential gene expression, gene sets analysis, etc. To see how to do the downstream analysis click here\n\n\n\n\n\n\nNetwork analysis of genes and ontologies\n\n\n\n\n\n\n\n\n\n\nMethylated DNA between Cancer and Normal\n\n\n\n\n\nEpigenetics is the study of how your behaviors and environment can cause changes that affect the way your genes work. Unlike genetic changes, epigenetic changes are reversible and do not change your DNA sequence, but they can change how your body reads a DNA sequence (CDC,2024). To learn how to analyze 450k Illumina Arrays and Whole Genome Bisulfite Sequencing data click here\n\n\n\n\n\nIn recent years the integration of Omics data has become important to understand the biological procesess more broadly. To learn how to use the GWAS Catalog with ChIP-Seq data or how to retrieve and analyze data from The Cancer Genome Atlas (TCGA) click here\n\n\n\n\n\n\nNumber of predited mutation effect per gene\n\n\n\n\n\n\n\n\n\n\nKEGG\n\n\n\n\n\nAn example of a proper use of interactive plots on a RNA-Seq data report can be founded here.\n\nThis repository doesn’t have any code, is only to showcase what is possible using Quarto and Bioconductor\n\n\n\n\n\n\nNetwork analysis of genes and ontologies\nMethylated DNA between Cancer and Normal\nNumber of predited mutation effect per gene\nKEGG"
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport seaborn as sns\n\n\nThe file gamma-ray.csv contains a small quantity of data collected from the Compton Gamma Ray Observatory, a satellite launched by NASA in 1991 (http://cossc.gsfc.nasa.gov/). For each of 100 sequential time intervals of variable lengths (given in seconds), the number of gamma rays originating in a particular area of the sky was recorded.\nFor this analysis, we would like to check the assumption that the emission rate is constant.\nFirst, let’s check the distribution of the number of gamma rays\n\n\nCode\ndf = pd.read_csv(\"gamma-ray.csv\", sep=\",\")\n\nsns.set_theme(style=\"darkgrid\")\nsns.displot(df,x=\"count\")\n\n\n\n\n\n\n\n\n\nThe number of these gamma rays is discrete and non-negative. We can assume that the gamma rays emerge independently of one another, and at constant rate in each time interval. Based on this assumption, a Poisson model is a good model assumption. Each observation \\(G_i\\) follows a poisson distribution with rate \\(\\lambda_i\\) times the time interval \\(t_i\\).\n\\[G_i \\thicksim Poisson(\\lambda_i * t_i)\\]\nTHe hypothesis that we wanna test is that the emission rate is constat so:\n\\(H_0: \\lambda_0 = \\lambda_1 = ... = \\lambda_{99}\\)\nAnd the alternative hypothesis is that the emisison rates are different:\n\\(H_A: \\lambda_i \\neq \\lambda_j\\) for some i and j\nThe most plausible parameter for \\(\\lambda\\) under the null hypothesis is the maximun likelihood estimator (MLE).\nGiven the poisson distribution, the derivation goes as follows:\n\\[f(G_0,G_1,...,G_{99} | \\lambda) = \\prod_{i=0}^{99} \\frac{e^{-\\lambda*t_i}*(\\lambda*t_i)^{G_i}}{G_i!}\\]\n\\[ln(f) = -\\lambda \\sum_{i=0}^{99} t_i + ln(\\lambda) \\sum_{i=0}^{99} G_i + ln(\\prod_{i=0}^{99} t_i^{G_i})) - ln(\\prod_{i=0}^{99} G_i!)\\]\nThen, we derivate w.r.t parameter \\(\\lambda\\) and set the derivative to 0 to find the optimal \\(\\lambda\\).\n\\[0 = -\\sum_{i=0}^{99}t_i + \\sum_{i=0}^{99} \\frac{G_i}{\\lambda}\\]\n\\[\\hat{\\lambda} = \\frac{\\sum_{i=0}^{99} G_i}{\\sum_{i=0}^{99} t_i}\\]\n\n\nCode\nlambda_null = df[\"count\"].sum() / df[\"seconds\"].sum()\n\nlambda_null\n\n\n0.0038808514969907496\n\n\nSimilarly, we can calculate a plausible value for the alternative hypothesis with MLE.\n\\[ln(f) = -\\sum_{i=0}^{99} \\lambda_i*t_i + \\sum_{i=0}^{99} G_i*ln(\\lambda_i) + ln(\\prod_{i=0}^{99} G_i*ln(t_i)) - ln(\\prod_{i=0}^{99} G_i!)\\]\nWE take the partial derivative w.r.t each parameter \\(\\lambda_i\\) to find the optimal \\(\\lambda_i\\)\n\\[0 = -t_i + \\frac{G_i}{\\lambda_i}\\]\n\\[\\hat{\\lambda_i} = \\frac{G_i}{t_i}\\]\n\n\nCode\nlambdas_alt = df[\"count\"]/df[\"seconds\"]\n\nlambdas_alt\n\n\n0     0.000000\n1     0.000000\n2     0.000000\n3     0.000000\n4     0.009804\n        ...   \n95    0.025840\n96    0.000000\n97    0.000000\n98    0.000000\n99    0.000000\nLength: 100, dtype: float64\n\n\nNow to test these hypotheses, we are gonna use the likelihood ratio test:\n\\[\\Lambda(x) = -2ln(\\frac{argmax f(G_0,G_1,...,G_{99}| \\lambda)}{argmax f(G_0,G_1,...,G_{99}| \\lambda_0,...,\\lambda_{99})})\\]\nwhich asymptotic distribution \\(X_{100-1 = 99}^{2}\\)\n\n\nCode\nlh_null = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambda_null)\n\nlh_alt = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambdas_alt)\n\nllh_test = -2*np.log(lh_null.prod() / lh_alt.prod())\n\nllh_test\n\n\n104.33272117042188\n\n\nWe can see the distribution of the chi-squared pdf at 99 degrees of freedom\n\n\nCode\nplot_Xs = np.arange(50,150,0.1)\nplt.plot(plot_Xs, sc.stats.chi2.pdf(plot_Xs, 99))\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd see the value that give a p-value of 0.05\n\n\nCode\n# We can calculate the Lambda that would give a p-value of 0.05 by using the inverse survival function\nsc.stats.chi2.isf(0.05, 99)\n\n\n123.22522145336181\n\n\nCalculating our p-value:\n\n\nCode\npvalue = sc.stats.chi2.sf(llh_test, 99)\nprint(pvalue)\n\n\n0.337398546433923\n\n\nWe can conclude that we cannot reject our null hypothesis, emission rate of gamma rays appear to be constant."
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#likelihood-ratio-test",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#likelihood-ratio-test",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport seaborn as sns\n\n\nThe file gamma-ray.csv contains a small quantity of data collected from the Compton Gamma Ray Observatory, a satellite launched by NASA in 1991 (http://cossc.gsfc.nasa.gov/). For each of 100 sequential time intervals of variable lengths (given in seconds), the number of gamma rays originating in a particular area of the sky was recorded.\nFor this analysis, we would like to check the assumption that the emission rate is constant.\nFirst, let’s check the distribution of the number of gamma rays\n\n\nCode\ndf = pd.read_csv(\"gamma-ray.csv\", sep=\",\")\n\nsns.set_theme(style=\"darkgrid\")\nsns.displot(df,x=\"count\")\n\n\n\n\n\n\n\n\n\nThe number of these gamma rays is discrete and non-negative. We can assume that the gamma rays emerge independently of one another, and at constant rate in each time interval. Based on this assumption, a Poisson model is a good model assumption. Each observation \\(G_i\\) follows a poisson distribution with rate \\(\\lambda_i\\) times the time interval \\(t_i\\).\n\\[G_i \\thicksim Poisson(\\lambda_i * t_i)\\]\nTHe hypothesis that we wanna test is that the emission rate is constat so:\n\\(H_0: \\lambda_0 = \\lambda_1 = ... = \\lambda_{99}\\)\nAnd the alternative hypothesis is that the emisison rates are different:\n\\(H_A: \\lambda_i \\neq \\lambda_j\\) for some i and j\nThe most plausible parameter for \\(\\lambda\\) under the null hypothesis is the maximun likelihood estimator (MLE).\nGiven the poisson distribution, the derivation goes as follows:\n\\[f(G_0,G_1,...,G_{99} | \\lambda) = \\prod_{i=0}^{99} \\frac{e^{-\\lambda*t_i}*(\\lambda*t_i)^{G_i}}{G_i!}\\]\n\\[ln(f) = -\\lambda \\sum_{i=0}^{99} t_i + ln(\\lambda) \\sum_{i=0}^{99} G_i + ln(\\prod_{i=0}^{99} t_i^{G_i})) - ln(\\prod_{i=0}^{99} G_i!)\\]\nThen, we derivate w.r.t parameter \\(\\lambda\\) and set the derivative to 0 to find the optimal \\(\\lambda\\).\n\\[0 = -\\sum_{i=0}^{99}t_i + \\sum_{i=0}^{99} \\frac{G_i}{\\lambda}\\]\n\\[\\hat{\\lambda} = \\frac{\\sum_{i=0}^{99} G_i}{\\sum_{i=0}^{99} t_i}\\]\n\n\nCode\nlambda_null = df[\"count\"].sum() / df[\"seconds\"].sum()\n\nlambda_null\n\n\n0.0038808514969907496\n\n\nSimilarly, we can calculate a plausible value for the alternative hypothesis with MLE.\n\\[ln(f) = -\\sum_{i=0}^{99} \\lambda_i*t_i + \\sum_{i=0}^{99} G_i*ln(\\lambda_i) + ln(\\prod_{i=0}^{99} G_i*ln(t_i)) - ln(\\prod_{i=0}^{99} G_i!)\\]\nWE take the partial derivative w.r.t each parameter \\(\\lambda_i\\) to find the optimal \\(\\lambda_i\\)\n\\[0 = -t_i + \\frac{G_i}{\\lambda_i}\\]\n\\[\\hat{\\lambda_i} = \\frac{G_i}{t_i}\\]\n\n\nCode\nlambdas_alt = df[\"count\"]/df[\"seconds\"]\n\nlambdas_alt\n\n\n0     0.000000\n1     0.000000\n2     0.000000\n3     0.000000\n4     0.009804\n        ...   \n95    0.025840\n96    0.000000\n97    0.000000\n98    0.000000\n99    0.000000\nLength: 100, dtype: float64\n\n\nNow to test these hypotheses, we are gonna use the likelihood ratio test:\n\\[\\Lambda(x) = -2ln(\\frac{argmax f(G_0,G_1,...,G_{99}| \\lambda)}{argmax f(G_0,G_1,...,G_{99}| \\lambda_0,...,\\lambda_{99})})\\]\nwhich asymptotic distribution \\(X_{100-1 = 99}^{2}\\)\n\n\nCode\nlh_null = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambda_null)\n\nlh_alt = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambdas_alt)\n\nllh_test = -2*np.log(lh_null.prod() / lh_alt.prod())\n\nllh_test\n\n\n104.33272117042188\n\n\nWe can see the distribution of the chi-squared pdf at 99 degrees of freedom\n\n\nCode\nplot_Xs = np.arange(50,150,0.1)\nplt.plot(plot_Xs, sc.stats.chi2.pdf(plot_Xs, 99))\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd see the value that give a p-value of 0.05\n\n\nCode\n# We can calculate the Lambda that would give a p-value of 0.05 by using the inverse survival function\nsc.stats.chi2.isf(0.05, 99)\n\n\n123.22522145336181\n\n\nCalculating our p-value:\n\n\nCode\npvalue = sc.stats.chi2.sf(llh_test, 99)\nprint(pvalue)\n\n\n0.337398546433923\n\n\nWe can conclude that we cannot reject our null hypothesis, emission rate of gamma rays appear to be constant."
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#regression-and-gradient-descent",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#regression-and-gradient-descent",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "Regression and Gradient Descent",
    "text": "Regression and Gradient Descent\n\nRegression\nWe are gonna use a dataset from General Motors about mortality. This data is from 59 US cities where it was studied the contribution of air pollution to mortality.\nThe dependen variable is the age adjusted mortality (Mortality). Also, the data includes variables measuring climate characteristics (JanTemp, JulyTemp, RelHum, Rain), demographic characteristics of the cities (Educ, Dens, NonWhite, WhiteCollar, Pop, House, Income), and variables recording the pollution potential of three diferent air pollutants (HC, NOx, SO2).\nLet’s load the data:\n\n\nCode\ndf = pd.read_csv(\"mortality.csv\", sep=\",\")\ndf.head()\n\n\n\n\n\n\n\n\n\nCity\nMortality\nJanTemp\nJulyTemp\nRelHum\nRain\nEduc\nDens\nNonWhite\nWhiteCollar\nPop\nHouse\nIncome\nHC\nNOx\nSO2\n\n\n\n\n0\nAkron, OH\n921.87\n27\n71\n59\n36\n11.4\n3243\n8.8\n42.6\n660328\n3.34\n29560\n21\n15\n59\n\n\n1\nAlbany-Schenectady-Troy, NY\n997.87\n23\n72\n57\n35\n11.0\n4281\n3.5\n50.7\n835880\n3.14\n31458\n8\n10\n39\n\n\n2\nAllentown, Bethlehem,PA-NJ\n962.35\n29\n74\n54\n44\n9.8\n4260\n0.8\n39.4\n635481\n3.21\n31856\n6\n6\n33\n\n\n3\nAtlanta, GA\n982.29\n45\n79\n56\n47\n11.1\n3125\n27.1\n50.2\n2138231\n3.41\n32452\n18\n8\n24\n\n\n4\nBaltimore, MD\n1071.29\n35\n77\n55\n43\n9.6\n6441\n24.4\n43.7\n2199531\n3.44\n32368\n43\n38\n206\n\n\n\n\n\n\n\nLet’s write two helper function that adds the intercept to the data and perform the ordinary least squares estimation based on the formula:\n\\[\\hat{\\beta}=(X^{T}X)^{-1}X^{T}y\\]\n\n\nCode\ndef add_intercept(X: np.array):\n  return np.concatenate((np.ones_like(X[:,:1]), X), axis=1)\n\ndef OLS(X: np.array, y: np.array):\n  return np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n\n\nAlso we need to declare the matrix X and the dependent variable y and convert them to numpy objects.\n\n\nCode\ny = df[\"Mortality\"]\nX = df.loc[:,\"JanTemp\":\"SO2\"]\n\n# Now lets add the intercept to X\n\nX = X.to_numpy()\ny = y.to_numpy()\n\n\nNow, can calculate the betas:\n\n\nCode\nbetas = OLS(add_intercept(X), y)\n\nbetas\n\n\narray([ 1.40003471e+03, -1.44112034e+00, -2.95025005e+00,  1.36009551e-01,\n        9.69507928e-01, -1.10466624e+01,  4.71706516e-03,  5.30271368e+00,\n       -1.49231882e+00,  3.40228581e-06, -3.80416445e+01, -4.25186359e-04,\n       -6.71247478e-01,  1.17850768e+00,  8.45829777e-02])\n\n\nGreat! This was an easy way to compute the OLS regression. We also can do the same with sklearn:\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\n\nreg.fit(add_intercept(X),y)\n\nprint(reg.intercept_, reg.coef_)\n\n\n1400.0347123466809 [ 0.00000000e+00 -1.44112034e+00 -2.95025005e+00  1.36009551e-01\n  9.69507928e-01 -1.10466624e+01  4.71706516e-03  5.30271368e+00\n -1.49231882e+00  3.40228581e-06 -3.80416445e+01 -4.25186359e-04\n -6.71247478e-01  1.17850768e+00  8.45829777e-02]\n\n\n\n\nGradient Descent\nWe also can do the regression task via optimization with the gradient descent algorithm. Let’s write some helper functions that will helps us to do the gradient descent algorithm.\n\n\nCode\ndef loss_fn(beta, X, y):\n  # (y - X beta)^T (y - X beta)\n  return np.sum(np.square(y - X.dot(beta)))\n\ndef loss_grad(beta, X, y):\n  # -2*(y - X beta)^T X\n  return -2*(y - X.dot(beta)).T.dot(X)\n\ndef gradient_step(beta, step_size, X, y):\n  loss, grads = loss_fn(beta, X, y), loss_grad(beta, X, y)\n  \n  beta = beta - step_size * grads.T\n  return loss, beta\n\ndef gradient_descent(X, y, step_size, precision, max_iter=10000, warn_max_iter=True):\n  beta = np.zeros_like(X[0])\n  \n  losses = [] # Array for recording the value of the loss over the iterations.\n  graceful = False\n  for _ in range(max_iter):\n    beta_last = beta # Save last values of beta for later stopping criterion\n    loss, beta = gradient_step(beta, step_size, X, y)\n    losses.append(loss)\n    # Use the euclidean norm of the difference between the new beta and the old beta as a stopping criteria\n    if np.sqrt(np.sum(np.square((beta - beta_last)))) &lt; precision:\n      graceful = True\n      break\n  if not graceful and warn_max_iter:\n    print(\"Reached max iterations.\")\n  return beta, np.array(losses)\n\n\nBefore applying the gradient descent, we need to standardize the data because the loss explote due to some outliers. &gt; You can try it without the standardization\n\n\nCode\n# Standardize the data\n\nX_scaled = (X - X.mean(axis=0))/X.std(axis=0, ddof=1)\nY_scaled = (y - y.mean())/y.std(ddof=1)\nX_inter = add_intercept(X_scaled)\n\n# Apply the gradient descent\n\nbeta_gd, losses = gradient_descent(X_inter, Y_scaled, 0.001, 1e-8, max_iter=10000)\nprint(\"BetaHat = {}\".format(beta_gd))\nprint(\"Final loss value = {}\".format(losses[-1]))\nplt.plot(range(len(losses)), losses)\nplt.title(\"Iteration vs Loss\")\nplt.show()\n\n# As a sanity check, we can also do the matrix inversion with OLS\nprint(\"Matrix inverse BetaHat = {}\".format(OLS(X_inter, Y_scaled)))\n\n\n\nReached max iterations.\nBetaHat = [ 9.99200722e-19 -2.34387032e-01 -2.17491727e-01  1.17191047e-02\n  1.79800652e-01 -1.50530629e-01  1.08943031e-01  7.64357603e-01\n -1.21223518e-01  8.40179286e-02 -1.11464155e-01 -3.04603568e-02\n -9.95161696e-01  8.80002697e-01  8.62630571e-02]\nFinal loss value = 13.809259921735789\nMatrix inverse BetaHat = [ 3.21984155e-17 -2.34377091e-01 -2.17496828e-01  1.17238925e-02\n  1.79754504e-01 -1.50544513e-01  1.08945825e-01  7.64348559e-01\n -1.21202349e-01  8.40325965e-02 -1.11479218e-01 -3.04687337e-02\n -9.96190618e-01  8.81043305e-01  8.61146149e-02]\n\n\n\n\n\n\n\n\n\nNote that the betas changed because we standardized the data, thats why at the end we are checking with the OLS function."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Likelihood Ratio Test, Regression and Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder Construction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]