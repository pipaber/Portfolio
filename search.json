[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Piero Palacios",
    "section": "",
    "text": "Hi! My name is Piero, and I’m a Biologist (with main formation in biotechnology) from Peru. I have specializations in data analysis and bioinformatics from Harvard University and MIT. Also, I love writing reproducible interactive reports using Quarto as a hobby.\nWithin my bioinformatic experience you can find RNA-Seq, Methyl-Seq, WGBS-Seq, Illumina Methylation Array and Multi-omics analyses. Also, I have experience on machine learning (OLS, ridge and lasso regression, SVM, Kernels, collaborative filtering, network analysis, high-dimensional reduction, etc) and deep learning models (Feed forward NN, Recurrent NN, Convolutional NN, Graph NN, Transformers, etc)."
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport seaborn as sns\n\n\nThe file gamma-ray.csv contains a small quantity of data collected from the Compton Gamma Ray Observatory, a satellite launched by NASA in 1991 (http://cossc.gsfc.nasa.gov/). For each of 100 sequential time intervals of variable lengths (given in seconds), the number of gamma rays originating in a particular area of the sky was recorded.\nFor this analysis, we would like to check the assumption that the emission rate is constant.\nFirst, let’s check the distribution of the number of gamma rays\n\n\nCode\ndf = pd.read_csv(\"gamma-ray.csv\", sep=\",\")\n\nsns.set_theme(style=\"darkgrid\")\nsns.displot(df,x=\"count\")\n\n\n\n\n\n\n\n\n\nThe number of these gamma rays is discrete and non-negative. We can assume that the gamma rays emerge independently of one another, and at constant rate in each time interval. Based on this assumption, a Poisson model is a good model assumption. Each observation \\(G_i\\) follows a poisson distribution with rate \\(\\lambda_i\\) times the time interval \\(t_i\\).\n\\[G_i \\thicksim Poisson(\\lambda_i * t_i)\\]\nTHe hypothesis that we wanna test is that the emission rate is constat so:\n\\(H_0: \\lambda_0 = \\lambda_1 = ... = \\lambda_{99}\\)\nAnd the alternative hypothesis is that the emisison rates are different:\n\\(H_A: \\lambda_i \\neq \\lambda_j\\) for some i and j\nThe most plausible parameter for \\(\\lambda\\) under the null hypothesis is the maximun likelihood estimator (MLE).\nGiven the poisson distribution, the derivation goes as follows:\n\\[f(G_0,G_1,...,G_{99} | \\lambda) = \\prod_{i=0}^{99} \\frac{e^{-\\lambda*t_i}*(\\lambda*t_i)^{G_i}}{G_i!}\\]\n\\[ln(f) = -\\lambda \\sum_{i=0}^{99} t_i + ln(\\lambda) \\sum_{i=0}^{99} G_i + ln(\\prod_{i=0}^{99} t_i^{G_i})) - ln(\\prod_{i=0}^{99} G_i!)\\]\nThen, we derivate w.r.t parameter \\(\\lambda\\) and set the derivative to 0 to find the optimal \\(\\lambda\\).\n\\[0 = -\\sum_{i=0}^{99}t_i + \\sum_{i=0}^{99} \\frac{G_i}{\\lambda}\\]\n\\[\\hat{\\lambda} = \\frac{\\sum_{i=0}^{99} G_i}{\\sum_{i=0}^{99} t_i}\\]\n\n\nCode\nlambda_null = df[\"count\"].sum() / df[\"seconds\"].sum()\n\nlambda_null\n\n\n0.0038808514969907496\n\n\nSimilarly, we can calculate a plausible value for the alternative hypothesis with MLE.\n\\[ln(f) = -\\sum_{i=0}^{99} \\lambda_i*t_i + \\sum_{i=0}^{99} G_i*ln(\\lambda_i) + ln(\\prod_{i=0}^{99} G_i*ln(t_i)) - ln(\\prod_{i=0}^{99} G_i!)\\]\nWE take the partial derivative w.r.t each parameter \\(\\lambda_i\\) to find the optimal \\(\\lambda_i\\)\n\\[0 = -t_i + \\frac{G_i}{\\lambda_i}\\]\n\\[\\hat{\\lambda_i} = \\frac{G_i}{t_i}\\]\n\n\nCode\nlambdas_alt = df[\"count\"]/df[\"seconds\"]\n\nlambdas_alt\n\n\n0     0.000000\n1     0.000000\n2     0.000000\n3     0.000000\n4     0.009804\n        ...   \n95    0.025840\n96    0.000000\n97    0.000000\n98    0.000000\n99    0.000000\nLength: 100, dtype: float64\n\n\nNow to test these hypotheses, we are gonna use the likelihood ratio test:\n\\[\\Lambda(x) = -2ln(\\frac{argmax f(G_0,G_1,...,G_{99}| \\lambda)}{argmax f(G_0,G_1,...,G_{99}| \\lambda_0,...,\\lambda_{99})})\\]\nwhich asymptotic distribution \\(X_{100-1 = 99}^{2}\\)\n\n\nCode\nlh_null = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambda_null)\n\nlh_alt = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambdas_alt)\n\nllh_test = -2*np.log(lh_null.prod() / lh_alt.prod())\n\nllh_test\n\n\n104.33272117042188\n\n\nWe can see the distribution of the chi-squared pdf at 99 degrees of freedom\n\n\nCode\nplot_Xs = np.arange(50,150,0.1)\nplt.plot(plot_Xs, sc.stats.chi2.pdf(plot_Xs, 99))\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd see the value that give a p-value of 0.05\n\n\nCode\n# We can calculate the Lambda that would give a p-value of 0.05 by using the inverse survival function\nsc.stats.chi2.isf(0.05, 99)\n\n\n123.22522145336181\n\n\nCalculating our p-value:\n\n\nCode\npvalue = sc.stats.chi2.sf(llh_test, 99)\nprint(pvalue)\n\n\n0.337398546433923\n\n\nWe can conclude that we cannot reject our null hypothesis, emission rate of gamma rays appear to be constant."
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#likelihood-ratio-test",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#likelihood-ratio-test",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy as sc\nimport seaborn as sns\n\n\nThe file gamma-ray.csv contains a small quantity of data collected from the Compton Gamma Ray Observatory, a satellite launched by NASA in 1991 (http://cossc.gsfc.nasa.gov/). For each of 100 sequential time intervals of variable lengths (given in seconds), the number of gamma rays originating in a particular area of the sky was recorded.\nFor this analysis, we would like to check the assumption that the emission rate is constant.\nFirst, let’s check the distribution of the number of gamma rays\n\n\nCode\ndf = pd.read_csv(\"gamma-ray.csv\", sep=\",\")\n\nsns.set_theme(style=\"darkgrid\")\nsns.displot(df,x=\"count\")\n\n\n\n\n\n\n\n\n\nThe number of these gamma rays is discrete and non-negative. We can assume that the gamma rays emerge independently of one another, and at constant rate in each time interval. Based on this assumption, a Poisson model is a good model assumption. Each observation \\(G_i\\) follows a poisson distribution with rate \\(\\lambda_i\\) times the time interval \\(t_i\\).\n\\[G_i \\thicksim Poisson(\\lambda_i * t_i)\\]\nTHe hypothesis that we wanna test is that the emission rate is constat so:\n\\(H_0: \\lambda_0 = \\lambda_1 = ... = \\lambda_{99}\\)\nAnd the alternative hypothesis is that the emisison rates are different:\n\\(H_A: \\lambda_i \\neq \\lambda_j\\) for some i and j\nThe most plausible parameter for \\(\\lambda\\) under the null hypothesis is the maximun likelihood estimator (MLE).\nGiven the poisson distribution, the derivation goes as follows:\n\\[f(G_0,G_1,...,G_{99} | \\lambda) = \\prod_{i=0}^{99} \\frac{e^{-\\lambda*t_i}*(\\lambda*t_i)^{G_i}}{G_i!}\\]\n\\[ln(f) = -\\lambda \\sum_{i=0}^{99} t_i + ln(\\lambda) \\sum_{i=0}^{99} G_i + ln(\\prod_{i=0}^{99} t_i^{G_i})) - ln(\\prod_{i=0}^{99} G_i!)\\]\nThen, we derivate w.r.t parameter \\(\\lambda\\) and set the derivative to 0 to find the optimal \\(\\lambda\\).\n\\[0 = -\\sum_{i=0}^{99}t_i + \\sum_{i=0}^{99} \\frac{G_i}{\\lambda}\\]\n\\[\\hat{\\lambda} = \\frac{\\sum_{i=0}^{99} G_i}{\\sum_{i=0}^{99} t_i}\\]\n\n\nCode\nlambda_null = df[\"count\"].sum() / df[\"seconds\"].sum()\n\nlambda_null\n\n\n0.0038808514969907496\n\n\nSimilarly, we can calculate a plausible value for the alternative hypothesis with MLE.\n\\[ln(f) = -\\sum_{i=0}^{99} \\lambda_i*t_i + \\sum_{i=0}^{99} G_i*ln(\\lambda_i) + ln(\\prod_{i=0}^{99} G_i*ln(t_i)) - ln(\\prod_{i=0}^{99} G_i!)\\]\nWE take the partial derivative w.r.t each parameter \\(\\lambda_i\\) to find the optimal \\(\\lambda_i\\)\n\\[0 = -t_i + \\frac{G_i}{\\lambda_i}\\]\n\\[\\hat{\\lambda_i} = \\frac{G_i}{t_i}\\]\n\n\nCode\nlambdas_alt = df[\"count\"]/df[\"seconds\"]\n\nlambdas_alt\n\n\n0     0.000000\n1     0.000000\n2     0.000000\n3     0.000000\n4     0.009804\n        ...   \n95    0.025840\n96    0.000000\n97    0.000000\n98    0.000000\n99    0.000000\nLength: 100, dtype: float64\n\n\nNow to test these hypotheses, we are gonna use the likelihood ratio test:\n\\[\\Lambda(x) = -2ln(\\frac{argmax f(G_0,G_1,...,G_{99}| \\lambda)}{argmax f(G_0,G_1,...,G_{99}| \\lambda_0,...,\\lambda_{99})})\\]\nwhich asymptotic distribution \\(X_{100-1 = 99}^{2}\\)\n\n\nCode\nlh_null = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambda_null)\n\nlh_alt = sc.stats.poisson.pmf(k=df[\"count\"], mu=lambdas_alt)\n\nllh_test = -2*np.log(lh_null.prod() / lh_alt.prod())\n\nllh_test\n\n\n104.33272117042188\n\n\nWe can see the distribution of the chi-squared pdf at 99 degrees of freedom\n\n\nCode\nplot_Xs = np.arange(50,150,0.1)\nplt.plot(plot_Xs, sc.stats.chi2.pdf(plot_Xs, 99))\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd see the value that give a p-value of 0.05\n\n\nCode\n# We can calculate the Lambda that would give a p-value of 0.05 by using the inverse survival function\nsc.stats.chi2.isf(0.05, 99)\n\n\n123.22522145336181\n\n\nCalculating our p-value:\n\n\nCode\npvalue = sc.stats.chi2.sf(llh_test, 99)\nprint(pvalue)\n\n\n0.337398546433923\n\n\nWe can conclude that we cannot reject our null hypothesis, emission rate of gamma rays appear to be constant."
  },
  {
    "objectID": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#regression-and-gradient-descent",
    "href": "posts/Regression_and_Gradient_Descent/regression_and_gradient_descent.html#regression-and-gradient-descent",
    "title": "Likelihood Ratio Test, Regression and Gradient Descent",
    "section": "Regression and Gradient Descent",
    "text": "Regression and Gradient Descent\n\nRegression\nWe are gonna use a dataset from General Motors about mortality. This data is from 59 US cities where it was studied the contribution of air pollution to mortality.\nThe dependen variable is the age adjusted mortality (Mortality). Also, the data includes variables measuring climate characteristics (JanTemp, JulyTemp, RelHum, Rain), demographic characteristics of the cities (Educ, Dens, NonWhite, WhiteCollar, Pop, House, Income), and variables recording the pollution potential of three diferent air pollutants (HC, NOx, SO2).\nLet’s load the data:\n\n\nCode\ndf = pd.read_csv(\"mortality.csv\", sep=\",\")\ndf.head()\n\n\n\n\n\n\n\n\n\n\nCity\nMortality\nJanTemp\nJulyTemp\nRelHum\nRain\nEduc\nDens\nNonWhite\nWhiteCollar\nPop\nHouse\nIncome\nHC\nNOx\nSO2\n\n\n\n\n0\nAkron, OH\n921.87\n27\n71\n59\n36\n11.4\n3243\n8.8\n42.6\n660328\n3.34\n29560\n21\n15\n59\n\n\n1\nAlbany-Schenectady-Troy, NY\n997.87\n23\n72\n57\n35\n11.0\n4281\n3.5\n50.7\n835880\n3.14\n31458\n8\n10\n39\n\n\n2\nAllentown, Bethlehem,PA-NJ\n962.35\n29\n74\n54\n44\n9.8\n4260\n0.8\n39.4\n635481\n3.21\n31856\n6\n6\n33\n\n\n3\nAtlanta, GA\n982.29\n45\n79\n56\n47\n11.1\n3125\n27.1\n50.2\n2138231\n3.41\n32452\n18\n8\n24\n\n\n4\nBaltimore, MD\n1071.29\n35\n77\n55\n43\n9.6\n6441\n24.4\n43.7\n2199531\n3.44\n32368\n43\n38\n206\n\n\n\n\n\n\n\n\nLet’s write two helper function that adds the intercept to the data and perform the ordinary least squares estimation based on the formula:\n\\[\\hat{\\beta}=(X^{T}X)^{-1}X^{T}y\\]\n\n\nCode\ndef add_intercept(X: np.array):\n  return np.concatenate((np.ones_like(X[:,:1]), X), axis=1)\n\ndef OLS(X: np.array, y: np.array):\n  return np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n\n\nAlso we need to declare the matrix X and the dependent variable y and convert them to numpy objects.\n\n\nCode\ny = df[\"Mortality\"]\nX = df.loc[:,\"JanTemp\":\"SO2\"]\n\n# Now lets add the intercept to X\n\nX = X.to_numpy()\ny = y.to_numpy()\n\n\nNow, can calculate the betas:\n\n\nCode\nbetas = OLS(add_intercept(X), y)\n\nbetas\n\n\narray([ 1.40003471e+03, -1.44112034e+00, -2.95025005e+00,  1.36009551e-01,\n        9.69507928e-01, -1.10466624e+01,  4.71706516e-03,  5.30271368e+00,\n       -1.49231882e+00,  3.40228581e-06, -3.80416445e+01, -4.25186359e-04,\n       -6.71247478e-01,  1.17850768e+00,  8.45829777e-02])\n\n\nGreat! This was an easy way to compute the OLS regression. We also can do the same with sklearn:\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\n\nreg.fit(add_intercept(X),y)\n\nprint(reg.intercept_, reg.coef_)\n\n\n1400.0347123466809 [ 0.00000000e+00 -1.44112034e+00 -2.95025005e+00  1.36009551e-01\n  9.69507928e-01 -1.10466624e+01  4.71706516e-03  5.30271368e+00\n -1.49231882e+00  3.40228581e-06 -3.80416445e+01 -4.25186359e-04\n -6.71247478e-01  1.17850768e+00  8.45829777e-02]\n\n\n\n\nGradient Descent\nWe also can do the regression task via optimization with the gradient descent algorithm. Let’s write some helper functions that will helps us to do the gradient descent algorithm.\n\n\nCode\ndef loss_fn(beta, X, y):\n  # (y - X beta)^T (y - X beta)\n  return np.sum(np.square(y - X.dot(beta)))\n\ndef loss_grad(beta, X, y):\n  # -2*(y - X beta)^T X\n  return -2*(y - X.dot(beta)).T.dot(X)\n\ndef gradient_step(beta, step_size, X, y):\n  loss, grads = loss_fn(beta, X, y), loss_grad(beta, X, y)\n  \n  beta = beta - step_size * grads.T\n  return loss, beta\n\ndef gradient_descent(X, y, step_size, precision, max_iter=10000, warn_max_iter=True):\n  beta = np.zeros_like(X[0])\n  \n  losses = [] # Array for recording the value of the loss over the iterations.\n  graceful = False\n  for _ in range(max_iter):\n    beta_last = beta # Save last values of beta for later stopping criterion\n    loss, beta = gradient_step(beta, step_size, X, y)\n    losses.append(loss)\n    # Use the euclidean norm of the difference between the new beta and the old beta as a stopping criteria\n    if np.sqrt(np.sum(np.square((beta - beta_last)))) &lt; precision:\n      graceful = True\n      break\n  if not graceful and warn_max_iter:\n    print(\"Reached max iterations.\")\n  return beta, np.array(losses)\n\n\nBefore applying the gradient descent, we need to standardize the data because the loss explote due to some outliers. &gt; You can try it without the standardization\n\n\nCode\n# Standardize the data\n\nX_scaled = (X - X.mean(axis=0))/X.std(axis=0, ddof=1)\nY_scaled = (y - y.mean())/y.std(ddof=1)\nX_inter = add_intercept(X_scaled)\n\n# Apply the gradient descent\n\nbeta_gd, losses = gradient_descent(X_inter, Y_scaled, 0.001, 1e-8, max_iter=10000)\nprint(\"BetaHat = {}\".format(beta_gd))\nprint(\"Final loss value = {}\".format(losses[-1]))\nplt.plot(range(len(losses)), losses)\nplt.title(\"Iteration vs Loss\")\nplt.show()\n\n# As a sanity check, we can also do the matrix inversion with OLS\nprint(\"Matrix inverse BetaHat = {}\".format(OLS(X_inter, Y_scaled)))\n\n\n\nReached max iterations.\nBetaHat = [ 9.99200722e-19 -2.34387032e-01 -2.17491727e-01  1.17191047e-02\n  1.79800652e-01 -1.50530629e-01  1.08943031e-01  7.64357603e-01\n -1.21223518e-01  8.40179286e-02 -1.11464155e-01 -3.04603568e-02\n -9.95161696e-01  8.80002697e-01  8.62630571e-02]\nFinal loss value = 13.809259921735789\nMatrix inverse BetaHat = [ 3.21984155e-17 -2.34377091e-01 -2.17496828e-01  1.17238925e-02\n  1.79754504e-01 -1.50544513e-01  1.08945825e-01  7.64348559e-01\n -1.21202349e-01  8.40325965e-02 -1.11479218e-01 -3.04687337e-02\n -9.96190618e-01  8.81043305e-01  8.61146149e-02]\n\n\n\n\n\n\n\n\n\nNote that the betas changed because we standardized the data, thats why at the end we are checking with the OLS function."
  },
  {
    "objectID": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html",
    "href": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html",
    "title": "Analysis of Hyper-parameters on Single Cell RNA-Seq Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nimport scanpy as sc\nfrom sklearn.model_selection import train_test_split\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff"
  },
  {
    "objectID": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html#getting-the-data",
    "href": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html#getting-the-data",
    "title": "Analysis of Hyper-parameters on Single Cell RNA-Seq Data",
    "section": "Getting the Data",
    "text": "Getting the Data\nWe will analyze three types of hyper-parameters: Perplexity on t-SNE and number of clusters chosen from an unsupervised method and how these affect the quality of the selected features.\nTo performs these three tasks we are gonna work with real data. Specifically, we’re gonna use scRNA-Seq data from a brain sample (GSM6900730). The data is available on this link.\nIn our case, let’s do a basic web scrapping code to get the data.\n\n\nCode\n# download the data and load it for the posterior analysis\nimport urllib.parse\nimport requests\nimport urllib\nfrom bs4 import BeautifulSoup\nimport os\nfrom ftplib import FTP\n\nurl = \"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM6900730\"\n\nresponse = requests.get(url)\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\ntable = soup.find('table')\nlinks = table.find_all('a')\n\ndownload_dir = os.getcwd()\n\nfor link in links:\n    # Get the URL of the link\n    link_url = link.get('href')\n    # Check if the link contains 'ftp'\n    if link_url and 'ftp' in link_url:\n        # Parse the FTP URL\n        link_url = urllib.parse.unquote(link_url)\n        parsed_url = urllib.parse.urlparse(link_url)\n        hostname = parsed_url.hostname\n        path = parsed_url.path\n        \n        # Connect to the FTP server\n        ftp = FTP(hostname)\n        ftp.login()\n        ftp.cwd(os.path.dirname(path))\n    \n        # Extract the file name from the path\n        file_name = os.path.basename(path)\n        local_file_path = os.path.join(download_dir, file_name)\n        \n        with open(local_file_path, \"wb\") as local_file:\n            ftp.retrbinary(f\"RETR {file_name}\", local_file.write)\n    \n        ftp.quit()\n        print(f\"Downloaded {file_name}\")\n        \n\n\nDownloaded GSM6900730_JLE16_B1_barcodes.tsv.gz\nDownloaded GSM6900730_JLE16_B1_features.tsv.gz\nDownloaded GSM6900730_JLE16_B1_matrix.mtx.gz\n\n\nNow, let’s load the data (is a sparse matrix) with scanpy:\n\n\nCode\nscdata = sc.read_10x_mtx('D:/Data Analysis Statistical Modeling and Computation in Applications/data/scrnaseq data/',\n                         var_names = 'gene_symbols',\n                         cache=False)\n\nX_sc = scdata.X.toarray()\nprint(X_sc.shape)\n\n\n(5154, 36601)\n\n\nOk, now we are gonna tranform the data with (log(x+1)) due to the presence of genes with extremely high magnitudes of expression in only a few cells.\n\n\nCode\nX_sc_transformed = np.log2(X_sc + 1)\n\n\nWe can use the principal components analysis to get the top components that explains the 85% variability of the data.\n\n\nCode\n# PCA\n\npca = PCA()\npca_axes = pca.fit_transform(X_sc_transformed)\n\n\ncsum = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(np.arange(0,X_sc_transformed.shape[0]),csum)\nplt.xlabel('Component #')\nplt.ylabel('% of Variance Explained')\nplt.title('% Variance explained by # Components')\nplt.show()\n\nprint(f'Variance explained 85%: {np.where(csum &gt;= 0.85)[0][0]}')\n\n\n\n\n\n\n\n\n\nVariance explained 85%: 1758\n\n\nLet’s save the number of components on a variable.\n\n\nCode\ntop_pca_components = np.where(csum &gt;= 0.85)[0][0]\n\n\n\nAnalysis of Perplexities\nFor the first part of the analysis, we are gonna analyze the following perplexities: 10,20,30,40,50 and see how this change the output of the t-SNE analysis.\n\n\nCode\n# let's write a helper function for the tsne\n\ndef tsne_plotter(data=None,per=40):\n\n    tsne = TSNE(n_components=2, perplexity=per)\n\n    x_tsne = tsne.fit_transform(data)\n\n    ax.scatter(x_tsne[:, 0], x_tsne[:, 1])\n    ax.set_xlabel('tSNE 1')\n    ax.set_ylabel('tSNE 2')\n    ax.set_title(f't-SNE Plot for log2\\nTransformed Data, Perplexity:{per}')\n\n\n\n\nCode\nperplexities = [10,20,30,40,50]\n\nnum_plots = len(perplexities)\nnum_rows = 2\nnum_cols = 3\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(10,8))\n\nfor i,perplexity in enumerate(perplexities):\n\n    row = i // num_cols\n    col = i % num_cols\n    ax = axes[row, col]\n\n    tsne_plotter(pca_axes[:,0:50], per= perplexity)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that increasing the perplexity values make the clusters more defined. This is in consonance with the definition of the perplexity parameter which says: “The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. The perplexity must be less than the number of samples”. Also, we can notice that from the perplexity value 30 to 50 there are more defined clusters; specifically, for perplexity 50 we see six more defined clusters so, this is the perplexity value chosen for the following analyses."
  },
  {
    "objectID": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html#number-of-clusters-chosen-from-an-unsupervised-method",
    "href": "posts/Naive_scRNA-Seq_analysis/naive_scrnaseq_analysis.html#number-of-clusters-chosen-from-an-unsupervised-method",
    "title": "Analysis of Hyper-parameters on Single Cell RNA-Seq Data",
    "section": "Number of Clusters Chosen from an Unsupervised Method",
    "text": "Number of Clusters Chosen from an Unsupervised Method\nFor this part of the analysis on the scRNA-Seq data, we are gonna use the Gaussian Mixture algorithm to get predictec clusters on the data. Then, using these clusters as “labels” we can perform a logistic regression and see how the test score changes when we change the number of clusters.\nFor this part, we are gonna use the first 50 principal components with the objective of a fast analysis.\n\n\nCode\nround(csum[:50][-1]*100,2)\n\n\n48.38\n\n\nWith 50 PC’s we achieve ~48% of explained variability.\nNow, let’s perform the clustering with the GMM model from 2 to 20 clusters. Also, let’s plot the BIC (Bayesian Information Criterion) and the Silhouette plot to see how many clusters we can select.\n\n\nCode\n## Clustering elbow plot GMM\n\nn_components_range = range(2, 21)\nbic_scores = []\n\nfor n_components in n_components_range:\n    gmm = GaussianMixture(n_components=n_components, random_state=0, init_params='kmeans')\n    gmm.fit(pca_axes[:,0:50])\n    # print(f'# comp: {n_components}')\n    bic_scores.append(gmm.bic(pca_axes[:,0:50]))\n\nplt.plot(n_components_range, bic_scores, marker='o', linestyle='-')\nplt.xlabel('Number of Components')\nplt.ylabel('BIC Score')\nplt.title('Elbow Plot for Gaussian Mixture Model')\nplt.xticks(n_components_range)\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe BIC score tells us that between 6 and 7 clusters can be a good choice.\n\n\nCode\n## Silhouette plot\n#\nn_components_range = range(2, 21)  # Start from 2 components (minimum required for silhouette score)\nsilhouette_scores = []\n\nfor n_components in n_components_range:\n    gmm = GaussianMixture(n_components=n_components, random_state=0, init_params='kmeans')\n    gmm.fit(pca_axes[:, 0:50])\n    # print(f'# comp: {n_components}')\n    labels = gmm.predict(pca_axes[:, 0:50])\n    silhouette_avg = silhouette_score(pca_axes[:, 0:50], labels)\n    silhouette_scores.append(silhouette_avg)\n\nplt.plot(n_components_range, silhouette_scores, marker='o', linestyle='-')\nplt.xlabel('Number of Components')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score Plot for Gaussian Mixture Model')\nplt.xticks(n_components_range)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe silhouette plot shows that 6, 11 and 18 can be good choices for number of clusters.\nLet’s work with 6, 7, 8, 11 and 18 clusters andd see how these perform.\n\nClusters Analysis\n\nNumber of clusters: 6Number of clusters: 7Number of clusters: 8Number of clusters: 11\n\n\n\n\nCode\nnp.random.seed(123)\n\ngmm = GaussianMixture(n_components=6, random_state=0, init_params='kmeans')\nclusters_gmm = gmm.fit_predict(pca_axes[:,0:50])\n\ntsne = TSNE(n_components=2, perplexity=50)\n\nx_tsne = tsne.fit_transform(pca_axes[:,0:50])\n\nplt.scatter(x_tsne[:,0], x_tsne[:,1], c=clusters_gmm)\nplt.xlabel('tSNE 1')\nplt.ylabel('tSNE 2')\nplt.title('t-SNE Plot for log2 Transformed Data with GMM')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_sc_transformed, clusters_gmm, test_size=0.2, random_state=42)\n\n\n\n\nCode\nlog_reg = LogisticRegressionCV(cv=10,Cs=[0.01,0.1,1,10],penalty=\"l1\",solver=\"liblinear\",multi_class=\"ovr\")\n\nlog_reg.fit(X_train,y_train)\n\nprint(f\"Train score: {log_reg.score(X_train,y_train)}\")\nprint(f\"Test score: {log_reg.score(X_test,y_test)}\")\n\n\nTrain score: 0.9932088285229203\nTest score: 0.9311348205625606\n\n\n\n\n\n\nCode\nnp.random.seed(123)\n\ngmm = GaussianMixture(n_components=7, random_state=0, init_params='kmeans')\nclusters_gmm = gmm.fit_predict(pca_axes[:,0:50])\n\ntsne = TSNE(n_components=2, perplexity=50)\n\nx_tsne = tsne.fit_transform(pca_axes[:,0:50])\n\nplt.scatter(x_tsne[:,0], x_tsne[:,1], c=clusters_gmm)\nplt.xlabel('tSNE 1')\nplt.ylabel('tSNE 2')\nplt.title('t-SNE Plot for log2 Transformed Data with GMM')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_sc_transformed, clusters_gmm, test_size=0.2, random_state=42)\n\n\n\n\nCode\nlog_reg = LogisticRegressionCV(cv=10,Cs=[0.01,0.1,1,10],penalty=\"l1\",solver=\"liblinear\",multi_class=\"ovr\")\n\nlog_reg.fit(X_train,y_train)\n\nprint(f\"Train score: {log_reg.score(X_train,y_train)}\")\nprint(f\"Test score: {log_reg.score(X_test,y_test)}\")\n\n\nTrain score: 0.9949066213921901\nTest score: 0.9321047526673133\n\n\n\n\n\n\nCode\nnp.random.seed(123)\n\ngmm = GaussianMixture(n_components=8, random_state=0, init_params='kmeans')\nclusters_gmm = gmm.fit_predict(pca_axes[:,0:50])\n\ntsne = TSNE(n_components=2, perplexity=50)\n\nx_tsne = tsne.fit_transform(pca_axes[:,0:50])\n\nplt.scatter(x_tsne[:,0], x_tsne[:,1], c=clusters_gmm)\nplt.xlabel('tSNE 1')\nplt.ylabel('tSNE 2')\nplt.title('t-SNE Plot for log2 Transformed Data with GMM')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_sc_transformed, clusters_gmm, test_size=0.2, random_state=42)\n\n\n\n\nCode\nlog_reg = LogisticRegressionCV(cv=10,Cs=[0.01,0.1,1,10],penalty=\"l1\",solver=\"liblinear\",multi_class=\"ovr\")\n\nlog_reg.fit(X_train,y_train)\n\nprint(f\"Train score: {log_reg.score(X_train,y_train)}\")\nprint(f\"Test score: {log_reg.score(X_test,y_test)}\")\n\n\nTrain score: 1.0\nTest score: 0.92725509214355\n\n\n\n\n\n\nCode\nnp.random.seed(123)\n\ngmm = GaussianMixture(n_components=11, random_state=0, init_params='kmeans')\nclusters_gmm = gmm.fit_predict(pca_axes[:,0:50])\n\ntsne = TSNE(n_components=2, perplexity=50)\n\nx_tsne = tsne.fit_transform(pca_axes[:,0:50])\n\nplt.scatter(x_tsne[:,0], x_tsne[:,1], c=clusters_gmm)\nplt.xlabel('tSNE 1')\nplt.ylabel('tSNE 2')\nplt.title('t-SNE Plot for log2 Transformed Data with GMM')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X_sc_transformed, clusters_gmm, test_size=0.2, random_state=42)\n\n\n\n\nCode\nlog_reg = LogisticRegressionCV(cv=10,Cs=[0.01,0.1,1,10],penalty=\"l1\",solver=\"liblinear\",multi_class=\"ovr\")\n\nlog_reg.fit(X_train,y_train)\n\nprint(f\"Train score: {log_reg.score(X_train,y_train)}\")\nprint(f\"Test score: {log_reg.score(X_test,y_test)}\")\n\n\nTrain score: 0.9970894979383944\nTest score: 0.8079534432589719"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Analysis of Hyper-parameters on Single Cell RNA-Seq Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder Construction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood Ratio Test, Regression and Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/test/aa.html",
    "href": "posts/test/aa.html",
    "title": "Under Construction",
    "section": "",
    "text": "Construction\n\n\n\n\n\nConstruction"
  },
  {
    "objectID": "bioinformatics.html",
    "href": "bioinformatics.html",
    "title": "Bioinformatics Projects",
    "section": "",
    "text": "The analysis of RNA-Seq data involves two distinct parts. The first one needs the use of servers or HPC (high performance computers) and has to do with quality control. pre-processing, alignment-mapping (usually with the STAR software) and counting (can be done using RSEM software). The second one is called downstream analysis and this part involves differential gene expression, gene sets analysis, etc. To see how to do the downstream analysis click here\n\n\n\n\n\n\nNetwork analysis of genes and ontologies\n\n\n\n\n\n\n\n\n\n\nMethylated DNA between Cancer and Normal\n\n\n\n\n\nEpigenetics is the study of how your behaviors and environment can cause changes that affect the way your genes work. Unlike genetic changes, epigenetic changes are reversible and do not change your DNA sequence, but they can change how your body reads a DNA sequence (CDC,2024). To learn how to analyze 450k Illumina Arrays and Whole Genome Bisulfite Sequencing data click here\n\n\n\n\n\nIn recent years the integration of Omics data has become important to understand the biological procesess more broadly. To learn how to use the GWAS Catalog with ChIP-Seq data or how to retrieve and analyze data from The Cancer Genome Atlas (TCGA) click here\n\n\n\n\n\n\nNumber of predited mutation effect per gene\n\n\n\n\n\n\n\n\n\n\nKEGG\n\n\n\n\n\nAn example of a proper use of interactive plots on a RNA-Seq data report can be founded here.\n\nThis repository doesn’t have any code, is only to showcase what is possible using Quarto and Bioconductor\n\n\n\n\n\n\nNetwork analysis of genes and ontologies\nMethylated DNA between Cancer and Normal\nNumber of predited mutation effect per gene\nKEGG"
  }
]